{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-26T08:17:13.464521Z",
     "start_time": "2025-02-26T08:17:09.557693Z"
    }
   },
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# 让图形在 Jupyter Notebook 中内嵌显示\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:17:13.507430Z",
     "start_time": "2025-02-26T08:17:13.465522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 打印当前 Python 版本信息\n",
    "print(sys.version_info)\n",
    "\n",
    "# 打印使用库的版本信息\n",
    "for module in mpl, np, pd, sklearn, torch :\n",
    "    print(module.__name__, module.__version__)\n",
    "\n",
    "# 设置 PyTorch 计算设备\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# 设置随机种子，以保证实验的可复现性\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)"
   ],
   "id": "7ea0c290f8099f6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=12, micro=3, releaselevel='final', serial=0)\n",
      "matplotlib 3.10.0\n",
      "numpy 2.0.2\n",
      "pandas 2.2.3\n",
      "sklearn 1.6.0\n",
      "torch 2.5.1+cu118\n",
      "cuda:0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据预处理",
   "id": "b934ba074a83015e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 截取数据集",
   "id": "ff7e9f083c198776"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:17:13.510392Z",
     "start_time": "2025-02-26T08:17:13.507430Z"
    }
   },
   "cell_type": "code",
   "source": "# !python Intercept_dataset_row.py",
   "id": "160d83a5a340bc85",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 划分数据集",
   "id": "19bb399cf83e7041"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:17:13.514385Z",
     "start_time": "2025-02-26T08:17:13.511396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # !python dataset_splitter.py \n",
    "# \n",
    "# # 目标文件夹\n",
    "# output_dir = \"./train_data_size_50000\"\n",
    "# \n",
    "# # 输入文件\n",
    "# src_file = f\"{output_dir}/UNv1.0.en-zh.en\"\n",
    "# tgt_file = f\"{output_dir}/UNv1.0.en-zh.zh\"\n",
    "# \n",
    "# # 输出文件路径\n",
    "# train_src, train_tgt = f\"{output_dir}/train.en\", f\"{output_dir}/train.zh\"\n",
    "# val_src, val_tgt = f\"{output_dir}/val.en\", f\"{output_dir}/val.zh\"\n",
    "# test_src, test_tgt = f\"{output_dir}/test.en\", f\"{output_dir}/test.zh\"\n",
    "# \n",
    "# # 读取所有行\n",
    "# with open(src_file, \"r\", encoding=\"utf-8\") as f_src, open(tgt_file, \"r\", encoding=\"utf-8\") as f_tgt:\n",
    "#     src_lines = f_src.readlines()\n",
    "#     tgt_lines = f_tgt.readlines()\n",
    "# \n",
    "# # 确保行数一致\n",
    "# assert len(src_lines) == len(tgt_lines), \"行数不匹配！\"\n",
    "# \n",
    "# data = list(zip(src_lines, tgt_lines))\n",
    "# \n",
    "# # 划分 90% 训练，5% 验证，5% 测试\n",
    "# total_size = len(data)\n",
    "# train_size = int(total_size * 0.9)\n",
    "# val_size = int(total_size * 0.05)\n",
    "# \n",
    "# train_data = data[:train_size]\n",
    "# val_data = data[train_size:train_size + val_size]\n",
    "# test_data = data[train_size + val_size:]\n",
    "# \n",
    "# # 保存数据\n",
    "# def save_data(file, dataset, idx):\n",
    "#     with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         # 使用 tqdm 显示进度条\n",
    "#         for line in tqdm(dataset, desc=f\"Saving {file}\", unit=\"line\"):\n",
    "#             f.write(line[idx])  # idx=0: en, idx=1: zh\n",
    "# \n",
    "# save_data(train_src, train_data, 0)\n",
    "# save_data(train_tgt, train_data, 1)\n",
    "# save_data(val_src, val_data, 0)\n",
    "# save_data(val_tgt, val_data, 1)\n",
    "# save_data(test_src, test_data, 0)\n",
    "# save_data(test_tgt, test_data, 1)\n",
    "# \n",
    "# print(f\"数据集拆分完成，所有文件已保存到 {output_dir} 目录！\")"
   ],
   "id": "392be2ee81a9c212",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Moses&Jieba 分词",
   "id": "c55cfe97251d5cc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:17:13.518537Z",
     "start_time": "2025-02-26T08:17:13.514385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# import jieba\n",
    "# from sacremoses import MosesTokenizer\n",
    "# from pathlib import Path\n",
    "# import subprocess\n",
    "# \n",
    "# # 配置目录路径\n",
    "# pair_dir = Path(\"./train_data_size_50000\")  # 双语数据目录\n",
    "# dest_dir = Path(\"./train_data_size_50000_cut\")  # 分词结果目录\n",
    "# \n",
    "# def moses_cut(in_file, out_file, lang):\n",
    "#     \"\"\"\n",
    "#     使用 MosesTokenizer 对输入文件的文本进行分词，并将分词后的结果保存到输出文件中。\n",
    "#     \"\"\"\n",
    "#     mt = MosesTokenizer(lang=lang)\n",
    "#     with open(out_file, \"w\", encoding=\"utf8\") as out_f, open(in_file, \"r\", encoding=\"utf8\") as f:\n",
    "#         lines = f.readlines()\n",
    "#         # 使用 tqdm 来显示处理进度\n",
    "#         for line in tqdm(lines, desc=f\"Moses tokenizing {in_file.name}\", unit=\"line\"):\n",
    "#             line = line.strip()\n",
    "#             if line:\n",
    "#                 cut_line = mt.tokenize(line, return_str=True)\n",
    "#                 out_f.write(cut_line.lower() + \"\\n\")\n",
    "# \n",
    "# def jieba_cut(in_file, out_file):\n",
    "#     \"\"\"\n",
    "#     使用 Jieba 对输入文件的文本进行分词，并将分词后的结果保存到输出文件中。\n",
    "#     \"\"\"\n",
    "#     with open(out_file, \"w\", encoding=\"utf8\") as out_f, open(in_file, \"r\", encoding=\"utf8\") as f:\n",
    "#         lines = f.readlines()\n",
    "#         # 使用 tqdm 来显示处理进度\n",
    "#         for line in tqdm(lines, desc=f\"Jieba tokenizing {in_file.name}\", unit=\"line\"):\n",
    "#             line = line.strip()\n",
    "#             if line:\n",
    "#                 cut_line = \" \".join(jieba.cut(line))\n",
    "#                 out_f.write(cut_line + \"\\n\")\n",
    "# \n",
    "# # 主程序\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 检查目标目录是否存在，如果没有则创建\n",
    "#     dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "# \n",
    "#     # 获取语言对文件所在的目录路径\n",
    "#     local_data_path = pair_dir\n",
    "#     data_dir = dest_dir\n",
    "# \n",
    "#     # 遍历 ['train', 'val', 'test'] 三个数据集模式，分别进行分词处理\n",
    "#     for mode in [\"train\", \"val\", \"test\"]:\n",
    "#         # 对源语言文本进行 Moses 分词处理\n",
    "#         moses_cut(\n",
    "#             local_data_path / f\"{mode}.en\",  # 英文源语言\n",
    "#             data_dir / f\"{mode}_src.cut.txt\",\n",
    "#             lang=\"en\"\n",
    "#         )\n",
    "#         print(f\"[{mode}] English text tokenization completed\")\n",
    "# \n",
    "#         # 对目标语言文本进行 Jieba 分词处理\n",
    "#         jieba_cut(\n",
    "#             local_data_path / f\"{mode}.zh\",  # 中文目标语言\n",
    "#             data_dir / f\"{mode}_trg.cut.txt\"\n",
    "#         )\n",
    "#         print(f\"[{mode}] Chinese text tokenization completed\")\n",
    "# \n",
    "#     print(\"Preprocessing completed.\")"
   ],
   "id": "f21651bdc4e211b4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### BPE 分词",
   "id": "b65b2bad5c8d9b72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:17:13.521430Z",
     "start_time": "2025-02-26T08:17:13.518537Z"
    }
   },
   "cell_type": "code",
   "source": "# !sh double_vocab_bpe_process.sh ./train_data_size_50000 ./train_data_size_50000_cut",
   "id": "b6a3ea0924ea37a5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DataLoader准备\n",
    "### LangPairDataset"
   ],
   "id": "9baf1472469ce7aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T08:17:13.580078Z",
     "start_time": "2025-02-26T08:17:13.521430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "dataset_path = \"./train_data_size_50000\"\n",
    "\n",
    "class LangPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    加载和处理双语数据集，并支持数据缓存。\n",
    "    \"\"\"\n",
    "    def __init__(self, mode=\"train\", max_length=128, overwrite_cache=False, data_dir=dataset_path):\n",
    "        \"\"\"\n",
    "        初始化数据集。\n",
    "        :param mode: 数据集模式（\"train\" 或 \"val\"）\n",
    "        :param max_length: 句子最大长度，超过则过滤\n",
    "        :param overwrite_cache: 是否覆盖缓存，默认为 False\n",
    "        :param data_dir: 数据目录\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)  # 数据存储路径\n",
    "        cache_path = self.data_dir / \".cache\" / f\"de2en_{mode}_{max_length}.npy\"  # 缓存路径\n",
    "        \n",
    "        if overwrite_cache or not cache_path.exists():  # 覆盖缓存或缓存不存在时重新处理\n",
    "            cache_path.parent.mkdir(parents=True, exist_ok=True)  # 创建缓存目录\n",
    "            \n",
    "            # 读取源语言和目标语言文件\n",
    "            with open(self.data_dir / f\"{mode}_src.bpe\", \"r\", encoding=\"utf8\") as file:\n",
    "                self.src = file.readlines()\n",
    "            \n",
    "            with open(self.data_dir / f\"{mode}_trg.bpe\", \"r\", encoding=\"utf8\") as file:\n",
    "                self.trg = file.readlines()\n",
    "            \n",
    "            filtered_src, filtered_trg = [], []  # 存放过滤后的句子\n",
    "            \n",
    "            # 过滤句子长度\n",
    "            for src, trg in zip(self.src, self.trg):\n",
    "                if len(src) <= max_length and len(trg) <= max_length:\n",
    "                    filtered_src.append(src.strip())  # 去除首尾空格\n",
    "                    filtered_trg.append(trg.strip())\n",
    "            \n",
    "            # 保存为 NumPy 数组并缓存\n",
    "            np.save(cache_path, {\"src\": np.array(filtered_src), \"trg\": np.array(filtered_trg)}, allow_pickle=True)\n",
    "            print(f\"save cache to {cache_path}\")\n",
    "        \n",
    "        else:  # 加载已有缓存\n",
    "            cache_dict = np.load(cache_path, allow_pickle=True).item()\n",
    "            print(f\"load {mode} dataset from {cache_path}\")\n",
    "            filtered_src = cache_dict[\"src\"]\n",
    "            filtered_trg = cache_dict[\"trg\"]\n",
    "        \n",
    "        self.src = filtered_src  # 源语言数据\n",
    "        self.trg = filtered_trg  # 目标语言数据\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        获取指定索引的源语言和目标语言句子。\n",
    "        \"\"\"\n",
    "        return self.src[index], self.trg[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集大小。\n",
    "        \"\"\"\n",
    "        return len(self.src)\n",
    "\n",
    "# 创建训练集和验证集对象\n",
    "train_ds = LangPairDataset(\"train\")\n",
    "val_ds = LangPairDataset(\"val\")"
   ],
   "id": "4892cd122f3d3f38",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_data_size_50000\\\\train_src.bpe'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 64\u001B[0m\n\u001B[0;32m     61\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msrc)\n\u001B[0;32m     63\u001B[0m \u001B[38;5;66;03m# 创建训练集和验证集对象\u001B[39;00m\n\u001B[1;32m---> 64\u001B[0m train_ds \u001B[38;5;241m=\u001B[39m \u001B[43mLangPairDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m val_ds \u001B[38;5;241m=\u001B[39m LangPairDataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[8], line 24\u001B[0m, in \u001B[0;36mLangPairDataset.__init__\u001B[1;34m(self, mode, max_length, overwrite_cache, data_dir)\u001B[0m\n\u001B[0;32m     21\u001B[0m cache_path\u001B[38;5;241m.\u001B[39mparent\u001B[38;5;241m.\u001B[39mmkdir(parents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# 创建缓存目录\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# 读取源语言和目标语言文件\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mmode\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_src.bpe\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msrc \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mreadlines()\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_dir \u001B[38;5;241m/\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_trg.bpe\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'train_data_size_50000\\\\train_src.bpe'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 例子：查看数据集大小\n",
    "print(len(train_ds))\n",
    "print(len(val_ds))\n",
    "for i in range(2):\n",
    "    print(\"source: {}\\ntarget: {}\".format(*train_ds[i]))\n",
    "    print(\"-\"*50)"
   ],
   "id": "a200bc6a2b8a82e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ### Tokenizer",
   "id": "9902f9c937b68aaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 构建英文和中文的word2idx和idx2word字典\n",
    "en_word2idx = {\n",
    "    \"[PAD]\": 0,     \n",
    "    \"[BOS]\": 1,     \n",
    "    \"[UNK]\": 2,     \n",
    "    \"[EOS]\": 3,     \n",
    "}\n",
    "zh_word2idx = {\n",
    "    \"[PAD]\": 0,     \n",
    "    \"[BOS]\": 1,     \n",
    "    \"[UNK]\": 2,     \n",
    "    \"[EOS]\": 3,     \n",
    "}\n",
    "\n",
    "# 反向索引\n",
    "en_idx2word = {value: key for key, value in en_word2idx.items()}\n",
    "zh_idx2word = {value: key for key, value in zh_word2idx.items()}\n",
    "\n",
    "# 分别加载英文和中文词表\n",
    "en_index = len(en_idx2word)\n",
    "zh_index = len(zh_idx2word)\n",
    "threshold = 1\n",
    "\n",
    "# 读取英文词表\n",
    "with open(f\"{dataset_path}/en.vocab\", \"r\", encoding=\"utf8\") as file:\n",
    "    for line in tqdm(file.readlines()):\n",
    "        token, counts = line.strip().split()\n",
    "        if int(counts) >= threshold:\n",
    "            en_word2idx[token] = en_index\n",
    "            en_idx2word[en_index] = token\n",
    "            en_index += 1\n",
    "\n",
    "# 读取中文词表  \n",
    "with open(f\"{dataset_path}/zh.vocab\", \"r\", encoding=\"utf8\") as file:\n",
    "    for line in tqdm(file.readlines()):\n",
    "        token, counts = line.strip().split()\n",
    "        if int(counts) >= threshold:\n",
    "            zh_word2idx[token] = zh_index\n",
    "            zh_idx2word[zh_index] = token\n",
    "            zh_index += 1"
   ],
   "id": "6320edf9d082e46e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, word2idx, idx2word, max_length=128, pad_idx=0, bos_idx=1, eos_idx=3, unk_idx=2):\n",
    "        \"\"\"\n",
    "        初始化 Tokenizer。\n",
    "        :param word2idx: 单词到索引的映射\n",
    "        :param idx2word: 索引到单词的映射\n",
    "        :param max_length: 最大句子长度，超出则截断\n",
    "        :param pad_idx: 填充 token 索引\n",
    "        :param bos_idx: 句子起始 token 索引\n",
    "        :param eos_idx: 句子结束 token 索引\n",
    "        :param unk_idx: 未知单词索引\n",
    "        \"\"\"\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.max_length = max_length\n",
    "        self.pad_idx = pad_idx\n",
    "        self.bos_idx = bos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.unk_idx = unk_idx\n",
    "\n",
    "    def encode(self, text_list, padding_first=False, add_bos=True, add_eos=True, return_mask=False):\n",
    "        \"\"\"\n",
    "        将文本列表编码为索引列表。\n",
    "        :param text_list: 文本列表，每个元素是一个单词列表\n",
    "        :param padding_first: 是否将 [PAD] 填充到句首\n",
    "        :param add_bos: 是否添加 [BOS] 起始符号\n",
    "        :param add_eos: 是否添加 [EOS] 结束符号\n",
    "        :param return_mask: 是否返回 mask\n",
    "        :return: 编码后的 input_ids 或 (input_ids, masks)\n",
    "        \"\"\"\n",
    "        max_length = min(self.max_length, add_eos + add_bos + max([len(text) for text in text_list]))\n",
    "        indices_list = []\n",
    "\n",
    "        for text in text_list:\n",
    "            indices = [self.word2idx.get(word, self.unk_idx) for word in text[:max_length - add_bos - add_eos]]\n",
    "            if add_bos: indices = [self.bos_idx] + indices\n",
    "            if add_eos: indices = indices + [self.eos_idx]\n",
    "\n",
    "            # 填充到 max_length\n",
    "            if padding_first:\n",
    "                indices = [self.pad_idx] * (max_length - len(indices)) + indices\n",
    "            else:\n",
    "                indices = indices + [self.pad_idx] * (max_length - len(indices))\n",
    "\n",
    "            indices_list.append(indices)\n",
    "\n",
    "        input_ids = torch.tensor(indices_list) # 转换为 tensor\n",
    "        masks = (input_ids == self.pad_idx).to(dtype=torch.int64)  # 生成 mask，标记 padding 部分\n",
    "\n",
    "        return input_ids if not return_mask else (input_ids, masks)\n",
    "\n",
    "    def decode(self, indices_list, remove_bos=True, remove_eos=True, remove_pad=True, split=False):\n",
    "        \"\"\"\n",
    "        解码索引列表为文本列表。\n",
    "        :param indices_list: 索引列表\n",
    "        :param remove_bos: 是否移除 [BOS] \n",
    "        :param remove_eos: 是否移除 [EOS]\n",
    "        :param remove_pad: 是否移除 [PAD]\n",
    "        :param split: 是否返回分词列表\n",
    "        :return: 解码后的文本列表\n",
    "        \"\"\"\n",
    "        text_list = []\n",
    "\n",
    "        for indices in indices_list:\n",
    "            text = []\n",
    "            for index in indices:\n",
    "                word = self.idx2word.get(index, \"[UNK]\")  # 获取单词\n",
    "                if remove_bos and word == \"[BOS]\": continue\n",
    "                if remove_eos and word == \"[EOS]\": break\n",
    "                if remove_pad and word == \"[PAD]\": break\n",
    "                text.append(word)\n",
    "\n",
    "            text_list.append(\" \".join(text) if not split else text)\n",
    "\n",
    "        return text_list"
   ],
   "id": "ff0473d08da12b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 创建英文和中文的tokenizer\n",
    "en_tokenizer = Tokenizer(word2idx=en_word2idx, idx2word=en_idx2word)\n",
    "zh_tokenizer = Tokenizer(word2idx=zh_word2idx, idx2word=zh_idx2word)\n",
    "\n",
    "# 输出词表大小\n",
    "en_vocab_size = len(en_word2idx)\n",
    "zh_vocab_size = len(zh_word2idx)\n",
    "print(\"en_vocab_size: {}\".format(en_vocab_size))  # 打印英文词表大小\n",
    "print(\"zh_vocab_size: {}\".format(zh_vocab_size))  # 打印中文词表大小"
   ],
   "id": "d020a4b0bcfeb51c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Transformer Batch Sampler",
   "id": "b570d392dee27856"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SampleInfo:\n",
    "    def __init__(self, i, lens):\n",
    "        \"\"\"\n",
    "        记录文本对的序号和长度信息。\n",
    "        \n",
    "        :param i: 文本对的序号。\n",
    "        :param lens: 文本对的长度，包含源语言和目标语言的长度。\n",
    "        \"\"\"\n",
    "        self.i = i\n",
    "        # 加一是为了考虑填充的特殊词元，lens[0] 和 lens[1] 分别表示源语言和目标语言的长度\n",
    "        self.max_len = max(lens[0], lens[1]) + 1\n",
    "        self.src_len = lens[0] + 1\n",
    "        self.trg_len = lens[1] + 1\n",
    "\n",
    "\n",
    "class TokenBatchCreator:\n",
    "    def __init__(self, batch_size):\n",
    "        \"\"\"\n",
    "        根据词元数目限制批量大小，并初始化批量存储结构。\n",
    "\n",
    "        :param batch_size: 批量的最大大小。\n",
    "        \"\"\"\n",
    "        self._batch = []  # 当前处理的样本\n",
    "        self.max_len = -1  # 当前批量的最大长度\n",
    "        self._batch_size = batch_size  # 批量大小限制\n",
    "\n",
    "    def append(self, info: SampleInfo):\n",
    "        \"\"\"\n",
    "        将样本信息添加到批量中。如果当前批量大小超过限制，则返回当前批量并创建新批量。\n",
    "\n",
    "        :param info: 包含文本对长度信息的 SampleInfo 对象。\n",
    "        :return: 当前批量样本，如果超过限制，则返回并重置批量，否则返回 None。\n",
    "        \"\"\"\n",
    "        cur_len = info.max_len  # 当前样本的最大长度\n",
    "        max_len = max(self.max_len, cur_len)  # 更新当前批量的最大长度\n",
    "\n",
    "        # 如果当前批量加入新样本后超过限制，返回当前批量并重置\n",
    "        if max_len * (len(self._batch) + 1) > self._batch_size:\n",
    "            self._batch, result = [], self._batch  # 保存当前样本并清空\n",
    "            self._batch.append(info)  # 新批量的第一条样本\n",
    "            self.max_len = cur_len  # 当前批量的最大长度为新样本的最大长度\n",
    "            return result\n",
    "        else:\n",
    "            self.max_len = max_len\n",
    "            self._batch.append(info)  # 将新样本加入当前批量\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def batch(self):\n",
    "        return self._batch"
   ],
   "id": "5c67080f63578e89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import BatchSampler\n",
    "import numpy as np\n",
    "\n",
    "class TransformerBatchSampler(BatchSampler):\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 batch_size,\n",
    "                 shuffle_batch=False,\n",
    "                 clip_last_batch=False,\n",
    "                 seed=0):\n",
    "        \"\"\"\n",
    "        批量采样器，用于按批次生成样本。\n",
    "\n",
    "        :param dataset: 数据集\n",
    "        :param batch_size: 每个批次的样本数量\n",
    "        :param shuffle_batch: 是否对批次进行洗牌\n",
    "        :param clip_last_batch: 是否裁剪最后一个批次的数据（如果样本数不足一个完整批次）\n",
    "        :param seed: 随机数种子，用于保证可重复性\n",
    "        \"\"\"\n",
    "        self._dataset = dataset\n",
    "        self._batch_size = batch_size\n",
    "        self._shuffle_batch = shuffle_batch\n",
    "        self._clip_last_batch = clip_last_batch\n",
    "        self._seed = seed\n",
    "        self._random = np.random\n",
    "        self._random.seed(seed)\n",
    "\n",
    "        self._sample_infos = []\n",
    "        # 创建样本信息列表，包含每个样本的索引和长度信息\n",
    "        for i, data in enumerate(self._dataset):\n",
    "            lens = [len(data[0]), len(data[1])]  # 计算样本的源语言和目标语言的长度\n",
    "            self._sample_infos.append(SampleInfo(i, lens)) # 保存为 [索引，样本长度]的格式\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        对数据集中的样本进行排序，并使用 TokenBatchCreator 生成批次。\n",
    "\n",
    "        排序规则：先按源语言长度排序，若源语言长度相同，再按目标语言长度排序。\n",
    "        生成的批次如果未裁剪最后一批，则将剩余样本组成最后一个批次。\n",
    "        如果需要洗牌，则对批次进行洗牌。\n",
    "        \n",
    "        :yield: 每个批次的样本在数据集中的索引\n",
    "        \"\"\"\n",
    "        # 按源语言和目标语言长度排序\n",
    "        infos = sorted(self._sample_infos, key=lambda x: (x.src_len, x.trg_len))\n",
    "        batch_infos = []\n",
    "        batch_creator = TokenBatchCreator(self._batch_size)  # 批量生成器\n",
    "\n",
    "        # 逐个样本加入批量生成器\n",
    "        for info in infos:\n",
    "            batch = batch_creator.append(info)\n",
    "            if batch is not None:\n",
    "                batch_infos.append(batch)  # 每当批次满足要求时，保存当前批次\n",
    "\n",
    "        # 如果未裁剪最后一个批次且有剩余样本，则将剩余样本作为最后一个批次\n",
    "        if not self._clip_last_batch and len(batch_creator.batch) != 0:\n",
    "            batch_infos.append(batch_creator.batch)\n",
    "\n",
    "        # 打乱批次顺序\n",
    "        if self._shuffle_batch:\n",
    "            self._random.shuffle(batch_infos)\n",
    "\n",
    "        # 记录批次数量\n",
    "        self.batch_number = len(batch_infos)\n",
    "\n",
    "        # 生成批次中的样本索引\n",
    "        for batch in batch_infos:\n",
    "            batch_indices = [info.i for info in batch]  # 获取当前批次的样本索引\n",
    "            yield batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回批次数量\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"batch_number\"):\n",
    "            return self.batch_number\n",
    "\n",
    "        # 如果没有记录批次数量，计算批次样本数量\n",
    "        batch_number = (len(self._dataset) + self._batch_size - 1) // self._batch_size\n",
    "        return batch_number"
   ],
   "id": "8455ce82888b56d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 创建一个 TransformerBatchSampler 实例，用于从训练数据集 train_ds 中按批次采样样本\n",
    "sampler = TransformerBatchSampler(\n",
    "    train_ds,  # 训练数据集，包含源语言和目标语言的样本\n",
    "    batch_size=4096,  # 每个批次包含的样本数量，设定为 4096\n",
    "    shuffle_batch=True  # 是否对批次进行洗牌，设定为 True，即每次迭代时都会打乱批次顺序\n",
    ")\n",
    "# 例子： 打印一个批次的样本索引\n",
    "for idx, batch in enumerate(sampler):\n",
    "    print(\"第{}批量的数据中含有文本对的索引是：{}，当前批次样本数量为：{}\".format(idx, batch, len(batch)))\n",
    "    break\n",
    "for i in batch:\n",
    "    print(f\"{i}: {train_ds[i]}\\n\");\n",
    "total_len = 0;"
   ],
   "id": "2ee2d7cfa13fccb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DataLoader",
   "id": "733964ae297ff8d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from functools import partial # 固定collate_fct的tokenizer参数\n",
    "def collate_fct(batch, en_tokenizer, zh_tokenizer):\n",
    "    # 分别对源语言(英文)和目标语言(中文)进行处理\n",
    "    src_words = [pair[0].split() for pair in batch]\n",
    "    trg_words = [pair[1].split() for pair in batch]\n",
    "\n",
    "    # 使用英文tokenizer处理源语言\n",
    "    encoder_inputs, encoder_inputs_mask = en_tokenizer.encode(\n",
    "        src_words, padding_first=False, add_bos=True, add_eos=True, return_mask=True\n",
    "    )\n",
    "\n",
    "    # 使用中文tokenizer处理目标语言\n",
    "    decoder_inputs = zh_tokenizer.encode(\n",
    "        trg_words, padding_first=False, add_bos=True, add_eos=False, return_mask=False,\n",
    "    )\n",
    "    \n",
    "    decoder_labels, decoder_labels_mask = zh_tokenizer.encode(\n",
    "        trg_words, padding_first=False, add_bos=False, add_eos=True, return_mask=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"encoder_inputs\": encoder_inputs.to(device=device),\n",
    "        \"encoder_inputs_mask\": encoder_inputs_mask.to(device=device),\n",
    "        \"decoder_inputs\": decoder_inputs.to(device=device),\n",
    "        \"decoder_labels\": decoder_labels.to(device=device),\n",
    "        \"decoder_labels_mask\": decoder_labels_mask.to(device=device),\n",
    "    }"
   ],
   "id": "3d0c30ee420d4ec7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 例子：查看编码后的批次数据\n",
    "#可以调大batch_size,来看最终的bleu，如果GPU内存不够，可以减小batch_size\n",
    "sampler = TransformerBatchSampler(train_ds, batch_size=256, shuffle_batch=True)\n",
    "#partial函数，固定collate_fct的tokenizer参数\n",
    "sample_dl = DataLoader(train_ds, batch_sampler=sampler, collate_fn=partial(collate_fct, en_tokenizer=en_tokenizer, zh_tokenizer=zh_tokenizer)) \n",
    "\n",
    "for batch in sample_dl:#外层是拿每个batch\n",
    "    for key, value in batch.items():#内层是拿每个batch里面是一个字典\n",
    "        print(key)\n",
    "        print(value)\n",
    "        print(value.shape)\n",
    "    break"
   ],
   "id": "e741b8e898efeb46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 定义模型",
   "id": "3a182c5d176092d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Embedding",
   "id": "8b04813e382404cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, config, vocab_size):\n",
    "        super().__init__()\n",
    "        # 获取配置中的超参数\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = config[\"d_model\"]\n",
    "        self.pad_idx = config[\"pad_idx\"]\n",
    "        dropout_rate = config[\"dropout\"]\n",
    "        self.max_length = config[\"max_length\"]\n",
    "\n",
    "        # 词嵌入层，padding_idx为pad的索引\n",
    "        self.word_embedding = nn.Embedding(self.vocab_size, self.hidden_size, padding_idx=self.pad_idx)\n",
    "        # 位置嵌入层，权重由get_positional_encoding计算得到\n",
    "        self.pos_embedding = nn.Embedding(\n",
    "            self.max_length,\n",
    "            self.hidden_size,\n",
    "            _weight=self.get_positional_encoding(self.max_length, self.hidden_size)\n",
    "        )\n",
    "        self.pos_embedding.weight.requires_grad_(False)  # 位置编码不可训练\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout层\n",
    "\n",
    "    def get_word_embedding_weights(self):\n",
    "        # 返回词嵌入层的权重\n",
    "        return self.word_embedding.weight\n",
    "\n",
    "    @classmethod\n",
    "    def get_positional_encoding(self, max_length, hidden_size):\n",
    "        \"\"\"\n",
    "        计算位置编码\n",
    "        使用正弦和余弦函数生成位置编码矩阵\n",
    "        \"\"\"\n",
    "        pe = torch.zeros(max_length, hidden_size)\n",
    "        position = torch.arange(0, max_length).unsqueeze(1)  # 位置索引\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, hidden_size, 2) * -(torch.log(torch.Tensor([10000.0])) / hidden_size)\n",
    "        )\n",
    "        # 填充位置编码矩阵\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数列为sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数列为cos\n",
    "        return pe\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        前向传播：词向量与位置编码加和\n",
    "        \"\"\"\n",
    "        seq_len = input_ids.shape[1]  # 序列长度\n",
    "        assert seq_len <= self.max_length, f\"序列长度超出最大限制 {self.max_length}\"\n",
    "\n",
    "        # 生成位置id\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device) # [seq_len]\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)  # [batch_size, seq_len]\n",
    "\n",
    "        # 获取词嵌入和位置编码\n",
    "        word_embeds = self.word_embedding(input_ids) # [batch_size, seq_len, hidden_size]\n",
    "        pos_embeds = self.pos_embedding(position_ids) # [batch_size, seq_len, hidden_size]\n",
    "        embeds = word_embeds + pos_embeds  # 加和词向量和位置编码\n",
    "        embeds = self.dropout(embeds)  # 应用dropout\n",
    "        return embeds  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "\n",
    "def plot_position_embedding(position_embedding):\n",
    "    \"\"\"\n",
    "    绘制位置编码矩阵\n",
    "    \"\"\"\n",
    "    plt.pcolormesh(position_embedding)  # 绘制矩阵\n",
    "    plt.xlabel('Depth')  # x轴为深度\n",
    "    plt.ylabel('Position')  # y轴为位置\n",
    "    plt.colorbar()  # 显示颜色条\n",
    "    plt.show()  # 显示图像"
   ],
   "id": "bd57c110b91e2fb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 例子：获取64个位置、128维词向量的位置编码矩阵并绘制\n",
    "position_embedding = TransformerEmbedding.get_positional_encoding(64, 128)\n",
    "plot_position_embedding(position_embedding)"
   ],
   "id": "49a6c190f5a92b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Transformer",
   "id": "b1cb752dfcdf7f80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Scaled Dot Product Attention",
   "id": "8f99de547b07abe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "@dataclass\n",
    "class AttentionOutput:\n",
    "    hidden_states: Tensor  # 注意力层的最终输出\n",
    "    attn_scores: Tensor    # 计算得到的注意力权重\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 获取配置中的超参数\n",
    "        self.hidden_size = config[\"d_model\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        \n",
    "        # 确保 hidden_size 可以被 num_heads 整除\n",
    "        assert self.hidden_size % self.num_heads == 0, \"Hidden size must be divisible by num_heads\"\n",
    "\n",
    "        self.head_dim = self.hidden_size // self.num_heads  # 每个头的维度\n",
    "\n",
    "        # 定义线性变换层\n",
    "        self.Wq = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.Wk = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.Wv = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.Wo = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "    def _split_heads(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        将输入张量拆分为多个头\n",
    "        \"\"\"\n",
    "        bs, seq_len, _ = x.shape\n",
    "        x = x.view(bs, seq_len, self.num_heads, self.head_dim)  # 调整形状\n",
    "        return x.permute(0, 2, 1, 3)  # 调整维度顺序\n",
    "\n",
    "    def _merge_heads(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        合并多个头的输出\n",
    "        \"\"\"\n",
    "        bs, _, seq_len, _ = x.shape\n",
    "        return x.permute(0, 2, 1, 3).reshape(bs, seq_len, self.hidden_size)\n",
    "\n",
    "    def forward(self, querys, keys, values, attn_mask=None) -> AttentionOutput:\n",
    "        \"\"\"\n",
    "        前向传播计算注意力机制\n",
    "        \"\"\"\n",
    "        # 线性变换并拆分为多个头\n",
    "        querys = self._split_heads(self.Wq(querys))  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        keys = self._split_heads(self.Wk(keys))      # [batch_size, num_heads, seq_len, head_dim]\n",
    "        values = self._split_heads(self.Wv(values))  # [batch_size, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # 计算 Q 和 K 之间的点积注意力分数\n",
    "        qk_logits = torch.matmul(querys, keys.mT)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "        # 如果提供了mask，应用它\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask[:, :, : querys.shape[-2], : keys.shape[-2]]  # 调整mask的大小\n",
    "            qk_logits += attn_mask * -1e9  # mask部分置为负无穷\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attn_scores = F.softmax(qk_logits / (self.head_dim**0.5), dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "        # 加权求和\n",
    "        embeds = torch.matmul(attn_scores, values)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # 合并头的输出并通过输出投影层\n",
    "        embeds = self.Wo(self._merge_heads(embeds))  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        return AttentionOutput(hidden_states=embeds, attn_scores=attn_scores)"
   ],
   "id": "7b0f0f610fd6a8ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Transformer Block",
   "id": "5f1c8ed655b4ba8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class TransformerBlockOutput:\n",
    "    # 当前块的输出（隐藏状态）和自/交叉注意力的分数\n",
    "    hidden_states: Tensor\n",
    "    self_attn_scores: Tensor\n",
    "    cross_attn_scores: Optional[Tensor] = None\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config, add_cross_attention=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 获取配置中的超参数\n",
    "        self.hidden_size = config[\"d_model\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        dropout_rate = config[\"dropout\"]\n",
    "        ffn_dim = config[\"dim_feedforward\"]\n",
    "        eps = config[\"layer_norm_eps\"]\n",
    "\n",
    "        # 自注意力机制\n",
    "        self.self_atten = MultiHeadAttention(config)\n",
    "        self.self_ln = nn.LayerNorm(self.hidden_size, eps=eps)\n",
    "        self.self_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 交叉注意力机制（仅解码器中使用）\n",
    "        if add_cross_attention:\n",
    "            self.cross_atten = MultiHeadAttention(config)\n",
    "            self.cross_ln = nn.LayerNorm(self.hidden_size, eps=eps)\n",
    "            self.cross_dropout = nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.cross_atten = None\n",
    "\n",
    "        # 前馈神经网络（FFN）\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, ffn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_dim, self.hidden_size),\n",
    "        )\n",
    "        self.ffn_ln = nn.LayerNorm(self.hidden_size, eps=eps)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, hidden_states, attn_mask=None, encoder_outputs=None, cross_attn_mask=None):\n",
    "        # 自注意力机制\n",
    "        self_atten_output = self.self_atten(\n",
    "            hidden_states, hidden_states, hidden_states, attn_mask\n",
    "        )\n",
    "        self_embeds = self.self_ln(\n",
    "            hidden_states + self.self_dropout(self_atten_output.hidden_states)\n",
    "        )\n",
    "\n",
    "        # 交叉注意力机制（解码器中）\n",
    "        if self.cross_atten is not None:\n",
    "            assert encoder_outputs is not None  # 使用交叉注意力时，必须传入编码器输出\n",
    "            cross_atten_output = self.cross_atten(\n",
    "                self_embeds, encoder_outputs, encoder_outputs, cross_attn_mask\n",
    "            )\n",
    "            cross_embeds = self.cross_ln(\n",
    "                self_embeds + self.cross_dropout(cross_atten_output.hidden_states)\n",
    "            )\n",
    "\n",
    "        # 前馈神经网络（FFN）\n",
    "        embeds = cross_embeds if self.cross_atten is not None else self_embeds\n",
    "        ffn_output = self.ffn(embeds)\n",
    "        embeds = self.ffn_ln(embeds + self.ffn_dropout(ffn_output))\n",
    "\n",
    "        # 返回 TransformerBlockOutput\n",
    "        return TransformerBlockOutput(\n",
    "            hidden_states=embeds,\n",
    "            self_attn_scores=self_atten_output.attn_scores,\n",
    "            cross_attn_scores=cross_atten_output.attn_scores if self.cross_atten is not None else None,\n",
    "        )"
   ],
   "id": "315d0c77f5c2652c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Encoder",
   "id": "f6c06503b7815611"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "@dataclass  # 存储TransformerEncoder的输出\n",
    "class TransformerEncoderOutput:\n",
    "    last_hidden_states: Tensor  # 最后一层的隐藏状态，包含上下文信息\n",
    "    attn_scores: List[Tensor]   # 每层的注意力分数，用于分析每层的关注点\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # 获取编码器层数\n",
    "        self.num_layers = config[\"num_encoder_layers\"]\n",
    "\n",
    "        # 创建包含多个TransformerBlock的列表，每个TransformerBlock代表编码器的一层\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(config) for _ in range(self.num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_inputs_embeds, attn_mask=None) -> TransformerEncoderOutput:\n",
    "        attn_scores = []  # 存储每层的自注意力分数\n",
    "        embeds = encoder_inputs_embeds  # 输入嵌入作为编码器的第一层输入\n",
    "\n",
    "        # 遍历每一层TransformerBlock\n",
    "        for layer in self.layers:\n",
    "            block_outputs = layer(embeds, attn_mask=attn_mask)  # 当前层输出\n",
    "            \n",
    "            embeds = block_outputs.hidden_states  # 更新下一层输入\n",
    "            attn_scores.append(block_outputs.self_attn_scores)  # 保存注意力分数\n",
    "\n",
    "        # 返回最后一层输出和所有层的注意力分数\n",
    "        return TransformerEncoderOutput(\n",
    "            last_hidden_states=embeds,  # 最后一层的输出\n",
    "            attn_scores=attn_scores  # 所有层的注意力分数\n",
    "        )"
   ],
   "id": "70aa1fe1c5d89641",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Decoder",
   "id": "138185edf3d182fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class TransformerDecoderOutput:\n",
    "    last_hidden_states: Tensor  # 最后一层的隐藏状态\n",
    "    self_attn_scores: List[Tensor]  # 每层的自注意力分数\n",
    "    cross_attn_scores: List[Tensor]  # 每层的交叉注意力分数\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 获取解码器层数\n",
    "        self.num_layers = config[\"num_decoder_layers\"]\n",
    "\n",
    "        # 创建多个TransformerBlock，每层都有交叉注意力机制\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(config, add_cross_attention=True) for _ in range(self.num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, decoder_inputs_embeds, encoder_outputs, attn_mask=None, cross_attn_mask=None) -> TransformerDecoderOutput:\n",
    "        self_attn_scores = []  # 存储每层的自注意力分数\n",
    "        cross_attn_scores = []  # 存储每层的交叉注意力分数\n",
    "        embeds = decoder_inputs_embeds  # 解码器输入嵌入\n",
    "\n",
    "        # 遍历每一层的TransformerBlock\n",
    "        for layer in self.layers:\n",
    "            # 前向传播，通过自注意力和交叉注意力机制\n",
    "            block_outputs = layer(\n",
    "                embeds,\n",
    "                attn_mask=attn_mask,\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                cross_attn_mask=cross_attn_mask,\n",
    "            )\n",
    "            embeds = block_outputs.hidden_states  # 更新输入为当前层输出\n",
    "\n",
    "            self_attn_scores.append(block_outputs.self_attn_scores)  # 保存自注意力分数\n",
    "            cross_attn_scores.append(block_outputs.cross_attn_scores)  # 保存交叉注意力分数\n",
    "\n",
    "        # 返回最后一层的隐藏状态和每层的注意力分数\n",
    "        return TransformerDecoderOutput(\n",
    "            last_hidden_states=embeds, \n",
    "            self_attn_scores=self_attn_scores,\n",
    "            cross_attn_scores=cross_attn_scores\n",
    "        )"
   ],
   "id": "40b6404562f388b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Mask",
   "id": "face90cdad3c5be4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    生成方形的后续掩码，屏蔽掉当前及之前的元素。\n",
    "    掩码位置为 True，未屏蔽的为 False。\n",
    "    \"\"\"\n",
    "    # 创建上三角矩阵并取反，形成掩码\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 0).transpose(-1, -2).bool()\n",
    "    return mask"
   ],
   "id": "c5e236846488087b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Transformer Model",
   "id": "954c5e61eaa76cbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class TransformerOutput:\n",
    "    logits: Tensor  # 模型的预测输出 logits\n",
    "    encoder_last_hidden_states: Tensor  # 编码器的最终隐藏状态\n",
    "    encoder_attn_scores: List[Tensor]  # 编码器各层的自注意力得分\n",
    "    decoder_last_hidden_states: Tensor  # 解码器的最终隐藏状态\n",
    "    decoder_self_attn_scores: List[Tensor]  # 解码器自注意力得分\n",
    "    decoder_cross_attn_scores: List[Tensor]  # 解码器交叉注意力得分\n",
    "    preds: Optional[Tensor] = None  # 推理时的预测结果\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 模型的各项配置初始化\n",
    "        self.hidden_size = config[\"d_model\"]  # Transformer模型中的隐藏层维度\n",
    "        self.num_encoder_layers = config[\"num_encoder_layers\"]  # 编码器层数\n",
    "        self.num_decoder_layers = config[\"num_decoder_layers\"]  # 解码器层数\n",
    "        self.pad_idx = config[\"pad_idx\"]  # padding 标记的索引\n",
    "        self.bos_idx = config[\"bos_idx\"]  # 句子开始标记的索引\n",
    "        self.eos_idx = config[\"eos_idx\"]  # 句子结束标记的索引\n",
    "        self.en_vocab_size = config[\"en_vocab_size\"]  # 英文词表大小\n",
    "        self.zh_vocab_size = config[\"zh_vocab_size\"]  # 中文词表大小\n",
    "        self.dropout_rate = config[\"dropout\"]  # Dropout比例\n",
    "        self.max_length = config[\"max_length\"]  # 最大序列长度\n",
    "\n",
    "        # 初始化源语言(英文)和目标语言(中文)的嵌入层\n",
    "        self.src_embedding = TransformerEmbedding(config, vocab_size=self.en_vocab_size)\n",
    "        self.trg_embedding = TransformerEmbedding(config, vocab_size=self.zh_vocab_size)\n",
    "        \n",
    "        # 输出层的线性变换，输出的维度为中文词表大小\n",
    "        self.linear = nn.Linear(self.hidden_size, self.zh_vocab_size)\n",
    "\n",
    "        # 初始化编码器和解码器\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.decoder = TransformerDecoder(config)\n",
    "        \n",
    "        # 权重初始化\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"初始化网络权重\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                # 使用 Xavier 均匀分布初始化权重\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\n",
    "        \"\"\"生成解码器的下三角掩码，用于自回归解码\"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 0).transpose(-1, -2).bool()  # 下三角掩码\n",
    "        return mask\n",
    "\n",
    "    def forward(self, encoder_inputs, decoder_inputs, encoder_inputs_mask=None) -> TransformerOutput:\n",
    "        \"\"\"\n",
    "        Transformer前向传播\n",
    "        1. 将输入的源语言和目标语言进行嵌入。\n",
    "        2. 通过编码器处理源语言输入。\n",
    "        3. 通过解码器生成目标语言的输出。\n",
    "        \"\"\"\n",
    "        if encoder_inputs_mask is None:\n",
    "            # 如果没有提供mask，则根据padding索引生成\n",
    "            encoder_inputs_mask = encoder_inputs.eq(self.pad_idx)\n",
    "        encoder_inputs_mask = encoder_inputs_mask.unsqueeze(1).unsqueeze(2)  # 扩展mask维度，以适应多头注意力\n",
    "\n",
    "        # 生成解码器掩码（防止信息泄漏）\n",
    "        look_ahead_mask = self.generate_square_subsequent_mask(decoder_inputs.shape[1]).unsqueeze(0).unsqueeze(0).to(decoder_inputs.device)\n",
    "        decoder_inputs_mask = decoder_inputs.eq(self.pad_idx).unsqueeze(1).unsqueeze(2) + look_ahead_mask\n",
    "\n",
    "        # 编码阶段：将源语言输入映射到嵌入空间\n",
    "        encoder_inputs_embeds = self.src_embedding(encoder_inputs)\n",
    "        encoder_outputs = self.encoder(encoder_inputs_embeds, encoder_inputs_mask)\n",
    "\n",
    "        # 解码阶段：将目标语言输入映射到嵌入空间\n",
    "        decoder_inputs_embeds = self.trg_embedding(decoder_inputs)\n",
    "        decoder_outputs = self.decoder(\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            encoder_outputs=encoder_outputs.last_hidden_states,\n",
    "            attn_mask=decoder_inputs_mask,\n",
    "            cross_attn_mask=encoder_inputs_mask,\n",
    "        )\n",
    "\n",
    "        # 将解码器的输出通过线性变换映射到目标语言的词表大小\n",
    "        logits = self.linear(decoder_outputs.last_hidden_states)\n",
    "\n",
    "        return TransformerOutput(\n",
    "            logits=logits,\n",
    "            encoder_last_hidden_states=encoder_outputs.last_hidden_states,\n",
    "            encoder_attn_scores=encoder_outputs.attn_scores,\n",
    "            decoder_last_hidden_states=decoder_outputs.last_hidden_states,\n",
    "            decoder_self_attn_scores=decoder_outputs.self_attn_scores,\n",
    "            decoder_cross_attn_scores=decoder_outputs.cross_attn_scores,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def infer(self, encoder_inputs, encoder_inputs_mask=None) -> Tensor:\n",
    "        \"\"\"推理：生成目标语言的翻译结果\"\"\"\n",
    "        if encoder_inputs_mask is None:\n",
    "            encoder_inputs_mask = encoder_inputs.eq(self.pad_idx)  # 根据padding生成mask\n",
    "        encoder_inputs_mask = encoder_inputs_mask.unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        # 生成解码器的掩码（自回归）\n",
    "        look_ahead_mask = self.generate_square_subsequent_mask(self.max_length).unsqueeze(0).unsqueeze(0).to(encoder_inputs.device)\n",
    "\n",
    "        # 编码阶段\n",
    "        encoder_inputs_embeds = self.src_embedding(encoder_inputs)\n",
    "        encoder_outputs = self.encoder(encoder_inputs_embeds)\n",
    "\n",
    "        # 解码阶段：生成目标语言翻译\n",
    "        decoder_inputs = torch.Tensor([self.bos_idx] * encoder_inputs.shape[0]).reshape(-1, 1).long().to(device=encoder_inputs.device)\n",
    "        for cur_len in tqdm(range(1, self.max_length + 1)):\n",
    "            decoder_inputs_embeds = self.trg_embedding(decoder_inputs)\n",
    "            decoder_outputs = self.decoder(\n",
    "                decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "                encoder_outputs=encoder_outputs.last_hidden_states,\n",
    "                attn_mask=look_ahead_mask[:, :, :cur_len, :cur_len],\n",
    "            )\n",
    "            logits = self.linear(decoder_outputs.last_hidden_states)\n",
    "            next_token = logits.argmax(dim=-1)[:, -1:]  # 选择下一个最可能的token\n",
    "            decoder_inputs = torch.cat([decoder_inputs, next_token], dim=-1)  # 将token加入解码输入中\n",
    "\n",
    "            # 如果所有样本的解码器输出达到结束标记，则停止解码\n",
    "            if all((decoder_inputs == self.eos_idx).sum(dim=-1) > 0):\n",
    "                break\n",
    "\n",
    "        return TransformerOutput(\n",
    "            preds=decoder_inputs[:, 1:],  # 排除开始标记，返回最终的预测结果\n",
    "            logits=logits,\n",
    "            encoder_last_hidden_states=encoder_outputs.last_hidden_states,\n",
    "            encoder_attn_scores=encoder_outputs.attn_scores,\n",
    "            decoder_last_hidden_states=decoder_outputs.last_hidden_states,\n",
    "            decoder_self_attn_scores=decoder_outputs.self_attn_scores,\n",
    "            decoder_cross_attn_scores=decoder_outputs.cross_attn_scores,\n",
    "        )"
   ],
   "id": "ce5c0ec31017265f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练",
   "id": "7ae5abc23f074b90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ### 损失函数",
   "id": "a073af9cd4ae401b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CrossEntropyWithPadding:\n",
    "    def __init__(self, config):\n",
    "        # 读取标签平滑参数\n",
    "        self.label_smoothing = config[\"label_smoothing\"]\n",
    "\n",
    "    def __call__(self, logits, labels, padding_mask=None):\n",
    "        bs, seq_len, nc = logits.shape  # 获取批次大小、序列长度和类别数\n",
    "\n",
    "        # 计算交叉熵损失，并应用标签平滑\n",
    "        loss = F.cross_entropy(logits.reshape(bs * seq_len, nc), labels.reshape(-1), reduction='none', label_smoothing=self.label_smoothing)\n",
    "\n",
    "        if padding_mask is None:\n",
    "            loss = loss.mean()  # 无padding时，返回平均损失\n",
    "        else:\n",
    "            # 处理padding掩码，忽略padding部分的损失\n",
    "            padding_mask = 1 - padding_mask.reshape(-1)\n",
    "            loss = torch.mul(loss, padding_mask).sum() / padding_mask.sum()  # 加权损失计算\n",
    "\n",
    "        return loss  # 返回损失值"
   ],
   "id": "ae5ea38f3d17d722",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 学习率衰减",
   "id": "649a38404d094bb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class NoamDecayScheduler:\n",
    "    def __init__(self, config):\n",
    "        # 初始化，获取模型维度和预热步数\n",
    "        self.d_model = config[\"d_model\"]\n",
    "        self.warmup_steps = config[\"warmup_steps\"]\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step += 1\n",
    "        arg1 = step ** (-0.5)  # 步数的衰减因子\n",
    "        arg2 = step * (self.warmup_steps ** (-1.5))  # 预热阶段的衰减因子\n",
    "        arg3 = self.d_model ** (-0.5)  # 模型维度的衰减因子\n",
    "\n",
    "        return arg3 * np.minimum(arg1, arg2)  # 返回衰减后的学习率\n",
    "\n",
    "# 创建一个实例\n",
    "temp_learning_rate_schedule = NoamDecayScheduler({\"d_model\": 512, \"warmup_steps\": 7000})"
   ],
   "id": "21206e2a73781aac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 例子：下面是学习率的设计图\n",
    "plt.plot(temp_learning_rate_schedule(np.arange(0, 40000)))\n",
    "plt.ylabel(\"Leraning rate\")\n",
    "plt.xlabel(\"Train step\")\n",
    "plt.show()"
   ],
   "id": "265b7b712040e2c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 优化器",
   "id": "acc577fe2fa11ec6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import Adam\n",
    "\n",
    "def get_optimizer(model, config):\n",
    "    base_lr = 0.1\n",
    "    beta1 = config[\"beta1\"]  # Adam 的 beta1\n",
    "    beta2 = config[\"beta2\"]  # Adam 的 beta2\n",
    "    eps = config[\"eps\"]\n",
    "\n",
    "    # 初始化Adam优化器\n",
    "    optimizer = Adam(model.parameters(), lr=base_lr, betas=(beta1, beta2), eps=eps)\n",
    "\n",
    "    # 初始化学习率调度器\n",
    "    lr_scheduler = NoamDecayScheduler(config)\n",
    "\n",
    "    # 使用 LambdaLR 调度器调整学习率\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lr_scheduler)\n",
    "\n",
    "    return optimizer, scheduler"
   ],
   "id": "57219a296d6cb65f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Callback",
   "id": "a606a4c9a6ac909d"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "训练过程中可以使用如下命令启动tensorboard服务。\n",
    "tensorboard --logdir=G:\\conference-text-translation-transformer\\ConfTrans\\runs --host 0.0.0.0 --port 8848   \n",
    "python -m tensorboard.main --logdir=G:\\conference-text-translation-transformer\\ConfTrans\\runs --host localhost --port 8848      "
   ],
   "id": "b6cff03b87ef9497"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class TensorBoardCallback:\n",
    "    def __init__(self, log_dir, flush_secs=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            log_dir (str): dir to write log.\n",
    "            flush_secs (int, optional): write to dsk each flush_secs seconds. Defaults to 10.\n",
    "        \"\"\"\n",
    "        self.writer = SummaryWriter(log_dir=log_dir, flush_secs=flush_secs)\n",
    "\n",
    "    def draw_model(self, model, input_shape):\n",
    "        self.writer.add_graph(model, input_to_model=torch.randn(input_shape))\n",
    "\n",
    "    def add_loss_scalars(self, step, loss, val_loss):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/loss\",\n",
    "            tag_scalar_dict={\"loss\": loss, \"val_loss\": val_loss},\n",
    "            global_step=step,\n",
    "            )\n",
    "\n",
    "    def add_acc_scalars(self, step, acc, val_acc):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/accuracy\",\n",
    "            tag_scalar_dict={\"accuracy\": acc, \"val_accuracy\": val_acc},\n",
    "            global_step=step,\n",
    "        )\n",
    "\n",
    "    def add_lr_scalars(self, step, learning_rate):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/learning_rate\",\n",
    "            tag_scalar_dict={\"learning_rate\": learning_rate},\n",
    "            global_step=step,\n",
    "\n",
    "        )\n",
    "\n",
    "    def __call__(self, step, **kwargs):\n",
    "        # add loss\n",
    "        loss = kwargs.pop(\"loss\", None)\n",
    "        val_loss = kwargs.pop(\"val_loss\", None)\n",
    "        if loss is not None and val_loss is not None:\n",
    "            self.add_loss_scalars(step, loss, val_loss)\n",
    "        # add acc\n",
    "        acc = kwargs.pop(\"acc\", None)\n",
    "        val_acc = kwargs.pop(\"val_acc\", None)\n",
    "        if acc is not None and val_acc is not None:\n",
    "            self.add_acc_scalars(step, acc, val_acc)\n",
    "        # add lr\n",
    "        learning_rate = kwargs.pop(\"lr\", None)\n",
    "        if learning_rate is not None:\n",
    "            self.add_lr_scalars(step, learning_rate)"
   ],
   "id": "54d5e212280338b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SaveCheckpointsCallback:\n",
    "    def __init__(self, save_dir, save_step=5000, save_best_only=True):\n",
    "        \"\"\"\n",
    "        Save checkpoints each save_epoch epoch.\n",
    "        We save checkpoint by epoch in this implementation.\n",
    "        Usually, training scripts with pytorch evaluating model and save checkpoint by step.\n",
    "\n",
    "        Args:\n",
    "            save_dir (str): dir to save checkpoint\n",
    "            save_epoch (int, optional): the frequency to save checkpoint. Defaults to 1.\n",
    "            save_best_only (bool, optional): If True, only save the best model or save each model at every epoch.\n",
    "        \"\"\"\n",
    "        self.save_dir = save_dir\n",
    "        self.save_step = save_step\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_metrics = - np.inf\n",
    "\n",
    "        # mkdir\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "\n",
    "    def __call__(self, step, state_dict, metric=None):\n",
    "        if step % self.save_step > 0:\n",
    "            return\n",
    "\n",
    "        if self.save_best_only:\n",
    "            assert metric is not None\n",
    "            if metric >= self.best_metrics:\n",
    "                # save checkpoints\n",
    "                torch.save(state_dict, os.path.join(self.save_dir, \"best.ckpt\"))\n",
    "                # update best metrics\n",
    "                self.best_metrics = metric\n",
    "        else:\n",
    "            torch.save(state_dict, os.path.join(self.save_dir, f\"{step}.ckpt\"))"
   ],
   "id": "671b8f33f2112547",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class EarlyStopCallback:\n",
    "    def __init__(self, patience=5, min_delta=0.01):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            patience (int, optional): Number of epochs with no improvement after which training will be stopped.. Defaults to 5.\n",
    "            min_delta (float, optional): Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute\n",
    "                change of less than min_delta, will count as no improvement. Defaults to 0.01.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_metric = - np.inf\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, metric):\n",
    "        if metric >= self.best_metric + self.min_delta:\n",
    "            # update best metric\n",
    "            self.best_metric = metric\n",
    "            # reset counter\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "    @property\n",
    "    def early_stop(self):\n",
    "        return self.counter >= self.patience\n"
   ],
   "id": "54d7f60ecbb77331",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training and Evaluation",
   "id": "424244df0f980222"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def evaluating(model, dataloader, loss_fct):\n",
    "    loss_list = []\n",
    "    for batch in dataloader:\n",
    "        encoder_inputs = batch[\"encoder_inputs\"]\n",
    "        encoder_inputs_mask = batch[\"encoder_inputs_mask\"]\n",
    "        decoder_inputs = batch[\"decoder_inputs\"]\n",
    "        decoder_labels = batch[\"decoder_labels\"]\n",
    "        decoder_labels_mask = batch[\"decoder_labels_mask\"]\n",
    "\n",
    "        # 前向计算\n",
    "        outputs = model(\n",
    "            encoder_inputs=encoder_inputs,\n",
    "            decoder_inputs=decoder_inputs,\n",
    "            encoder_inputs_mask=encoder_inputs_mask\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # 计算损失\n",
    "        loss = loss_fct(logits, decoder_labels, padding_mask=decoder_labels_mask)\n",
    "        loss_list.append(loss.cpu().item())\n",
    "\n",
    "    return np.mean(loss_list)  # 返回平均损失"
   ],
   "id": "8c355fdf1c702f4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 训练\n",
    "def training(\n",
    "    model,\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    epoch,\n",
    "    loss_fct,\n",
    "    optimizer,\n",
    "    scheduler=None,\n",
    "    tensorboard_callback=None,\n",
    "    save_ckpt_callback=None,\n",
    "    early_stop_callback=None,\n",
    "    eval_step=500,\n",
    "    ):\n",
    "    record_dict = {\n",
    "        \"train\": [],\n",
    "        \"val\": []\n",
    "    }\n",
    "\n",
    "    val_loss = 0.0\n",
    "    global_step = 1\n",
    "    model.train()\n",
    "    with tqdm(total=epoch * len(train_dl)) as pbar:\n",
    "        for epoch_id in range(epoch):\n",
    "            # training\n",
    "            for batch in train_dl:\n",
    "                encoder_inputs = batch[\"encoder_inputs\"]\n",
    "                encoder_inputs_mask = batch[\"encoder_inputs_mask\"]\n",
    "                decoder_inputs = batch[\"decoder_inputs\"]\n",
    "                decoder_labels = batch[\"decoder_labels\"]\n",
    "                decoder_labels_mask = batch[\"decoder_labels_mask\"]\n",
    "                # 梯度清空\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 前向计算\n",
    "                outputs = model(\n",
    "                    encoder_inputs=encoder_inputs,\n",
    "                    decoder_inputs=decoder_inputs,\n",
    "                    encoder_inputs_mask=encoder_inputs_mask\n",
    "                    )\n",
    "                logits = outputs.logits\n",
    "                loss = loss_fct(logits, decoder_labels, padding_mask=decoder_labels_mask)\n",
    "\n",
    "                # 梯度回传\n",
    "                loss.backward()\n",
    "\n",
    "                # 调整优化器，包括学习率的变动等\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step() # 更新学习率\n",
    "\n",
    "                loss = loss.cpu().item()\n",
    "                # record\n",
    "                record_dict[\"train\"].append({\n",
    "                    \"loss\": loss, \"step\": global_step\n",
    "                })\n",
    "\n",
    "                # evaluating\n",
    "                if global_step % eval_step == 0:\n",
    "                    model.eval()\n",
    "                    val_loss = evaluating(model, val_dl, loss_fct)\n",
    "                    record_dict[\"val\"].append({\n",
    "                        \"loss\": val_loss, \"step\": global_step\n",
    "                    })\n",
    "                    model.train()\n",
    "\n",
    "                    # 1. 使用 tensorboard 可视化\n",
    "                    cur_lr = optimizer.param_groups[0][\"lr\"] if scheduler is None else scheduler.get_last_lr()[0]\n",
    "                    if tensorboard_callback is not None:\n",
    "                        tensorboard_callback(\n",
    "                            global_step,\n",
    "                            loss=loss, val_loss=val_loss,\n",
    "                            lr=cur_lr,\n",
    "                            )\n",
    "\n",
    "                    # 2. 保存模型权重 save model checkpoint\n",
    "                    if save_ckpt_callback is not None:\n",
    "                        save_ckpt_callback(global_step, model.state_dict(), metric=-val_loss)\n",
    "\n",
    "                    # 3. 早停 Early Stop\n",
    "                    if early_stop_callback is not None:\n",
    "                        early_stop_callback(-val_loss)\n",
    "                        if early_stop_callback.early_stop:\n",
    "                            print(f\"Early stop at epoch {epoch_id} / global_step {global_step}\")\n",
    "                            return record_dict\n",
    "\n",
    "                # udate step\n",
    "                global_step += 1\n",
    "                pbar.update(1)\n",
    "            pbar.set_postfix({\"epoch\": epoch_id, \"loss\": loss, \"val_loss\": val_loss})\n",
    "\n",
    "    return record_dict"
   ],
   "id": "172f33f11b543d77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Config",
   "id": "bcc0b43b76c9975c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 配置模型的超参数\n",
    "config = {\n",
    "    \"bos_idx\": 1,  # 句子起始标记 (Beginning of Sentence)\n",
    "    \"eos_idx\": 3,  # 句子结束标记 (End of Sentence)\n",
    "    \"pad_idx\": 0,  # 填充标记 (Padding Index)\n",
    "    \"en_vocab_size\": len(en_word2idx),  # 英文词表大小\n",
    "    \"zh_vocab_size\": len(zh_word2idx),  # 中文词表大小\n",
    "    \"max_length\": 128,  # 句子最大长度\n",
    "    \"d_model\": 512,  # Transformer 词向量的维度\n",
    "    \"dim_feedforward\": 2048,  # 前馈神经网络（FFN）的隐藏层大小\n",
    "    \"dropout\": 0.2,  # dropout 率，用于防止过拟合\n",
    "    \"layer_norm_eps\": 1e-6,  # 层归一化 (Layer Normalization) 时的 epsilon，防止除零错误\n",
    "    \"num_heads\": 8,  # 多头注意力机制的头数\n",
    "    \"num_decoder_layers\": 6,  # 解码器层数\n",
    "    \"num_encoder_layers\": 6,  # 编码器层数\n",
    "    \"label_smoothing\": 0.1,  # 交叉熵损失中的标签平滑系数\n",
    "    \"beta1\": 0.9,  # Adam 优化器的 beta1 参数（用于一阶矩估计）\n",
    "    \"beta2\": 0.98,  # Adam 优化器的 beta2 参数（用于二阶矩估计）\n",
    "    \"eps\": 1e-9,  # Adam 优化器的 epsilon，防止除零错误\n",
    "    \"warmup_steps\": 3000,  # 学习率预热步数\n",
    "    \"share_embedding\": False,  # 是否在编码器和解码器之间共享词向量\n",
    "}\n",
    "\n",
    "def get_dl(dataset, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    获取数据加载器 (DataLoader)\n",
    "\n",
    "    参数：\n",
    "    dataset: 训练或验证数据集\n",
    "    batch_size: 批次大小\n",
    "    shuffle: 是否对批次进行随机排序（默认 True）\n",
    "\n",
    "    返回：\n",
    "    DataLoader 对象\n",
    "    \"\"\"\n",
    "    # 使用 Transformer 任务自定义的批次采样器，确保 batch 内的句子长度一致\n",
    "    sampler = TransformerBatchSampler(dataset, batch_size=batch_size, shuffle_batch=shuffle)\n",
    "    # 使用 DataLoader 进行数据加载，collate_fn 用于处理不同长度的句子\n",
    "    sample_dl = DataLoader(dataset, batch_sampler=sampler, collate_fn=partial(collate_fct, en_tokenizer=en_tokenizer, zh_tokenizer=zh_tokenizer))\n",
    "    return sample_dl\n",
    "\n",
    "# 构建训练集和验证集\n",
    "train_ds = LangPairDataset(\"train\", max_length=config[\"max_length\"])  # 训练数据集\n",
    "val_ds = LangPairDataset(\"val\", max_length=config[\"max_length\"])  # 验证数据集\n",
    "\n",
    "# 训练的批次大小\n",
    "batch_size = 2048\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dl = get_dl(train_ds, batch_size=batch_size, shuffle=True)  # 训练数据加载器\n",
    "val_dl = get_dl(val_ds, batch_size=batch_size, shuffle=False)  # 验证数据加载器"
   ],
   "id": "5b30ebeb8dd189ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "# 指定目录\n",
    "save_dir = \"./checkpoints/model_dropout_0_2_bpe_30000_mini_epoch20_{}\".format(\"share\" if config[\"share_embedding\"] else \"not_share\") \n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 保存路径\n",
    "config_file_path = os.path.join(save_dir, \"Config.txt\")\n",
    "\n",
    "# 将 config 字典写入文件\n",
    "with open(config_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "print(f\"Config has been saved to {config_file_path}\")"
   ],
   "id": "797a8e1aaeea3802",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#计算模型参数量\n",
    "model = TransformerModel(config)\n",
    "print(f\"模型参数量: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ],
   "id": "d054d3344121700a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epoch = 20\n",
    "\n",
    "# 初始化模型\n",
    "model = TransformerModel(config)\n",
    "\n",
    "# 定义损失函数，采用交叉熵损失\n",
    "loss_fct = CrossEntropyWithPadding(config)\n",
    "\n",
    "# 定义优化器和学习率调度器，采用 Adam 优化器\n",
    "optimizer, scheduler = get_optimizer(model, config)\n",
    "\n",
    "# 创建 tensorboard 可视化目录\n",
    "if not os.path.exists(\"runs\"):\n",
    "    os.mkdir(\"runs\")\n",
    "\n",
    "# exp_name \n",
    "exp_name = \"model_dropout_0_2_bpe_30000_mini_epoch20_{}\".format(\"share\" if config[\"share_embedding\"] else \"not_share\")\n",
    "tensorboard_callback = TensorBoardCallback(f\"runs/{exp_name}\")\n",
    "\n",
    "# 创建保存检查点的目录\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.makedirs(\"checkpoints\")\n",
    "save_ckpt_callback = SaveCheckpointsCallback(f\"checkpoints/{exp_name}\", save_step=500, save_best_only=True)\n",
    "\n",
    "# 早停机制\n",
    "early_stop_callback = EarlyStopCallback(patience=8)\n",
    "\n",
    "# 将模型转移到设备\n",
    "model = model.to(device)"
   ],
   "id": "9ca2b7400ff45b3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 开始训练",
   "id": "669922b10120027"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "record = training(\n",
    "    model,\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    epoch,\n",
    "    loss_fct,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    tensorboard_callback= tensorboard_callback,\n",
    "    save_ckpt_callback=save_ckpt_callback,\n",
    "    early_stop_callback=early_stop_callback,\n",
    "    eval_step=500\n",
    "    )"
   ],
   "id": "99bfe77a2ff190e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ### 损失率折线图",
   "id": "f0f3cfe01971f620"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_losses = [item['loss'] for item in record['train']]\n",
    "train_steps = [item['step'] for item in record['train']]\n",
    "val_losses = [item['loss'] for item in record['val']]\n",
    "val_steps = [item['step'] for item in record['val']]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_steps, train_losses, label='Training Loss')\n",
    "plt.plot(val_steps, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "8347bcbc76e528a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 推理",
   "id": "39580a6bee41fe05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 加载模型",
   "id": "fd5ded5540042285"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load(f\"./checkpoints/{exp_name}/best.ckpt\", map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "# 加载 Transformer 模型\n",
    "model = TransformerModel(config)  # 初始化 Transformer 模型\n",
    "model.load_state_dict(state_dict)  # 加载预训练模型的参数"
   ],
   "id": "563c9ac7eda78ee6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 计算 BLEU 评分",
   "id": "37426f5b72478607"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义损失函数（交叉熵损失，带有标签平滑）\n",
    "loss_fct = CrossEntropyWithPadding(config)\n",
    "\n",
    "# 加载测试数据集\n",
    "test_ds = LangPairDataset(\"test\", max_length=128, data_dir=f\"./{dataset_path}\")  # 测试数据集\n",
    "test_dl = DataLoader(test_ds, batch_size=1, collate_fn=partial(collate_fct, en_tokenizer=en_tokenizer, zh_tokenizer=zh_tokenizer))  # 数据加载器\n",
    "\n",
    "# 迁移模型到计算设备（CPU/GPU）\n",
    "model = model.to(device)\n",
    "model.eval()  # 设置模型为评估模式，防止 dropout 或 batchnorm 影响推理\n",
    "\n",
    "# 创建平滑函数实例\n",
    "smoothing_function = SmoothingFunction().method1  # 使用method1平滑\n",
    "\n",
    "# 定义数据收集字典\n",
    "collect = {}  # 用于存储测试集中的样本及其损失\n",
    "loss_collect = []  # 用于收集所有样本的损失\n",
    "\n",
    "predictions = []  # 存储所有预测结果\n",
    "answers = []  # 存储所有真实标签\n",
    "bleu1_scores = []  # 存储 BLEU-1 评分\n",
    "bleu2_scores = []  # 存储 BLEU-2 评分\n",
    "bleu3_scores = []  # 存储 BLEU-3 评分\n",
    "bleu4_scores = []  # 存储 BLEU-4 评分\n",
    "\n",
    "# 遍历测试集\n",
    "for idx, batch in tqdm(enumerate(test_dl), total=len(test_dl), desc=\"Evaluating\", unit=\"batch\"):\n",
    "    encoder_inputs = batch[\"encoder_inputs\"]  # 编码器输入（源语言）\n",
    "    encoder_inputs_mask = batch[\"encoder_inputs_mask\"]  # 源输入的填充掩码\n",
    "    decoder_inputs = batch[\"decoder_inputs\"]  # 解码器输入（目标语言）\n",
    "    decoder_labels = batch[\"decoder_labels\"]  # 目标真实标签（用于计算损失）\n",
    "\n",
    "    # 进行前向传播，获取 Transformer 输出\n",
    "    outputs = model(\n",
    "        encoder_inputs=encoder_inputs,\n",
    "        decoder_inputs=decoder_inputs,\n",
    "        encoder_inputs_mask=encoder_inputs_mask\n",
    "    )\n",
    "\n",
    "    # 计算交叉熵损失\n",
    "    loss = loss_fct(outputs.logits, decoder_labels)\n",
    "\n",
    "    # 获取预测结果：取每个时间步上最大概率的词索引\n",
    "    preds = outputs.logits.argmax(dim=-1)  # 预测序列形状为 [1, seq_len]\n",
    "\n",
    "    # 将预测索引转换为实际的文本句子\n",
    "    preds = zh_tokenizer.decode(preds.cpu().numpy())  # ['预测句子']\n",
    "\n",
    "    # 将真实标签转换为文本句子\n",
    "    decoder_labels = zh_tokenizer.decode(decoder_labels.cpu().numpy())  # ['标签句子']\n",
    "\n",
    "    # 计算不同n-gram的BLEU评分\n",
    "    bleu1 = sentence_bleu([decoder_labels[0].split()], preds[0].split(), weights=(1, 0, 0, 0), smoothing_function=smoothing_function)\n",
    "    bleu2 = sentence_bleu([decoder_labels[0].split()], preds[0].split(), weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing_function)\n",
    "    bleu3 = sentence_bleu([decoder_labels[0].split()], preds[0].split(), weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing_function)\n",
    "    bleu4 = sentence_bleu([decoder_labels[0].split()], preds[0].split(), weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)\n",
    "\n",
    "    bleu1_scores.append(bleu1)  # 存储 BLEU-1 分数\n",
    "    bleu2_scores.append(bleu2)  # 存储 BLEU-2 分数\n",
    "    bleu3_scores.append(bleu3)  # 存储 BLEU-3 分数\n",
    "    bleu4_scores.append(bleu4)  # 存储 BLEU-4 分数\n",
    "\n",
    "    # 记录样本信息，包括损失、输入、目标、预测\n",
    "    collect[idx] = {\n",
    "        \"loss\": loss.item(),  # 当前样本的损失\n",
    "        \"src_inputs\": encoder_inputs,  # 源语言输入\n",
    "        \"trg_inputs\": decoder_inputs,  # 目标语言输入\n",
    "        \"mask\": encoder_inputs_mask,  # 源语言的填充掩码\n",
    "        \"trg_labels\": decoder_labels,  # 真实目标文本\n",
    "        \"preds\": preds  # 预测文本\n",
    "    }\n",
    "\n",
    "    # 记录损失\n",
    "    loss_collect.append(loss.item())\n",
    "\n",
    "# 按照损失大小对收集的数据进行排序（从低到高）\n",
    "collect = sorted(collect.items(), key=lambda x: x[1][\"loss\"])\n",
    "\n",
    "# 输出测试集平均损失\n",
    "print(f\"testing loss: {np.array(loss_collect).mean()}\")\n",
    "\n",
    "# 输出各个n-gram BLEU评分的平均值\n",
    "print(f\"Average BLEU-1 score: {sum(bleu1_scores) / len(bleu1_scores)}\")\n",
    "print(f\"Average BLEU-2 score: {sum(bleu2_scores) / len(bleu2_scores)}\")\n",
    "print(f\"Average BLEU-3 score: {sum(bleu3_scores) / len(bleu3_scores)}\")\n",
    "print(f\"Average BLEU-4 score: {sum(bleu4_scores) / len(bleu4_scores)}\")"
   ],
   "id": "6c3a34b0b93a3f6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Translating",
   "id": "4935fbd334a4b7a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from sacremoses import MosesDetokenizer, MosesTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import matplotlib\n",
    "\n",
    "class Translator:\n",
    "    def __init__(self, model, en_tokenizer, zh_tokenizer, dataset_path=\"./train_data_size_50000\"):\n",
    "        \"\"\"\n",
    "        初始化翻译器类，包含 BPE 处理、分词、去标记化和模型推理功能。\n",
    "        \"\"\"\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        \n",
    "        # 英文和中文的 BPE 规则文件\n",
    "        self.en_bpe_rules = self.dataset_path / \"bpe.en.30000\"\n",
    "        self.zh_bpe_rules = self.dataset_path / \"bpe.zh.30000\"\n",
    "        \n",
    "        # 英文和中文的词汇表文件\n",
    "        self.en_vocab = self.dataset_path / \"en.vocab\"\n",
    "        self.zh_vocab = self.dataset_path / \"zh.vocab\"\n",
    "\n",
    "        # 初始化 Moses 分词器和去标记化器\n",
    "        self.mose_tokenizer = MosesTokenizer(lang=\"en\")  # 英文分词器\n",
    "        self.mose_detokenizer = MosesDetokenizer(lang=\"zh\")  # 中文去标记化器\n",
    "\n",
    "        # 设置模型和评估模式\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "        # 设置英文和中文的 tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.zh_tokenizer = zh_tokenizer\n",
    "        \n",
    "        # 正则模式，用于去除 BPE 标记\n",
    "        self.pattern = re.compile(r'(@@ )|(@@ ?$)')\n",
    "\n",
    "    def apply_bpe(self, input_file, output_file, bpe_rules, vocab_file):\n",
    "        \"\"\"\n",
    "        使用 subword-nmt 的 apply-bpe 命令对文件进行 BPE 分词。\n",
    "        \n",
    "        :param input_file: 输入文件路径\n",
    "        :param output_file: 输出文件路径\n",
    "        :param bpe_rules: BPE 规则文件路径\n",
    "        :param vocab_file: 词汇表文件路径\n",
    "        \"\"\"\n",
    "        try:\n",
    "            subprocess.run([\n",
    "                \"subword-nmt\", \"apply-bpe\",\n",
    "                \"-c\", str(bpe_rules),\n",
    "                \"--vocabulary\", str(vocab_file),\n",
    "                \"--vocabulary-threshold\", \"50\",\n",
    "                \"-i\", str(input_file),\n",
    "                \"-o\", str(output_file)\n",
    "            ], check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise RuntimeError(f\"Failed to apply BPE: {e}\")\n",
    "\n",
    "    def draw_attention_map(self, attn_scores, cross_attn_scores, src_words_list, trg_words_list):\n",
    "        \"\"\"\n",
    "        绘制 Transformer 模型的注意力（Attention）热力图，包括自注意力和交叉注意力。\n",
    "        \"\"\"\n",
    "        # 设置中文字体\n",
    "        matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # 指定黑体\n",
    "        matplotlib.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "        \n",
    "        # 验证输入张量的形状\n",
    "        assert len(attn_scores.shape) == 3, f\"attn_scores 形状应为 [num_heads, trg_len, trg_len]，但得到了 {attn_scores.shape}\"\n",
    "        attn_scores = attn_scores[:, :len(trg_words_list), :len(trg_words_list)]  # 截取目标序列长度\n",
    "        assert len(cross_attn_scores.shape) == 3, f\"cross_attn_scores 形状应为 [num_heads, trg_len, src_len]，但得到了 {cross_attn_scores.shape}\"\n",
    "        cross_attn_scores = cross_attn_scores[:, :len(trg_words_list), :len(src_words_list)]  # 截取源序列长度\n",
    "\n",
    "        num_heads, trg_len, src_len = cross_attn_scores.shape\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 8), constrained_layout=True)\n",
    "        grid = fig.add_gridspec(trg_len, trg_len + src_len, wspace=0.1, hspace=0.1)\n",
    "\n",
    "\n",
    "        # 绘制自注意力热力图\n",
    "        self_map = fig.add_subplot(grid[:, :trg_len])\n",
    "        self_map.matshow(attn_scores.mean(dim=0), cmap='viridis')  # 平均值作为热力图\n",
    "        self_map.set_yticks(range(trg_len), trg_words_list, fontsize=10)\n",
    "        self_map.set_xticks(range(trg_len), [\"[BOS]\"] + trg_words_list[:-1], rotation=90)\n",
    "\n",
    "        # 绘制交叉注意力热力图\n",
    "        cross_map = fig.add_subplot(grid[:, trg_len:])\n",
    "        cross_map.matshow(cross_attn_scores.mean(dim=0), cmap='viridis')\n",
    "        cross_map.set_yticks(range(trg_len), [])\n",
    "        cross_map.set_xticks(range(src_len), src_words_list, rotation=90)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def __call__(self, sentence_list, heads_list=None, layer_idx=-1):\n",
    "        \"\"\"\n",
    "        执行翻译任务，并可选地绘制注意力热力图。\n",
    "        \"\"\"\n",
    "        # 英文句子预处理\n",
    "        sentence_list = [\" \".join(self.mose_tokenizer.tokenize(s.lower())) for s in sentence_list]\n",
    "        print(sentence_list)\n",
    "        \n",
    "        # 将句子列表写入临时文件\n",
    "        temp_input_file = self.dataset_path / \"temp_input.txt\"\n",
    "        temp_output_file = self.dataset_path / \"temp_output.bpe\"\n",
    "        \n",
    "        # 写入英文句子\n",
    "        with open(temp_input_file, \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(\"\\n\".join(sentence_list))\n",
    "\n",
    "        # 对英文句子进行 BPE 分词\n",
    "        self.apply_bpe(temp_input_file, temp_output_file, self.en_bpe_rules, self.en_vocab)\n",
    "\n",
    "        # 读取 BPE 分词后的结果\n",
    "        with open(temp_output_file, \"r\", encoding=\"utf8\") as f:\n",
    "            en_tokens_list = [line.strip().split() for line in f]\n",
    "        print(\"English tokens1:\", en_tokens_list)\n",
    "\n",
    "        # 使用英文tokenizer处理输入\n",
    "        encoder_input, attn_mask = self.en_tokenizer.encode(en_tokens_list, add_bos=True, add_eos=True, return_mask=True)\n",
    "        print(\"English tokens2:\", encoder_input) \n",
    "        print(\"Attention mask:\", attn_mask)\n",
    "        \n",
    "        encoder_input = torch.Tensor(encoder_input).to(dtype=torch.int64)\n",
    "        print(\"Encoder input:\", encoder_input)\n",
    "       \n",
    "        # 执行模型推理\n",
    "        outputs = self.model.infer(encoder_inputs=encoder_input, encoder_inputs_mask=attn_mask)\n",
    "\n",
    "        preds = outputs.preds.numpy()\n",
    "        print(\"Predictions:\", preds)\n",
    "\n",
    "        # 获取模型的注意力分数\n",
    "        attn_scores = outputs.decoder_self_attn_scores[layer_idx]  # 自注意力\n",
    "        cross_attn_scores = outputs.decoder_cross_attn_scores[layer_idx]  # 交叉注意力\n",
    "\n",
    "        # 使用中文tokenizer解码输出\n",
    "        trg_decoded = self.zh_tokenizer.decode(preds, split=True, remove_eos=False, remove_bos=False, remove_pad=False)\n",
    "        # 使用英文tokenizer解码输入\n",
    "        src_decoded = self.en_tokenizer.decode(encoder_input.numpy(), split=True, remove_bos=False, remove_eos=False)\n",
    "\n",
    "        # 绘制注意力图\n",
    "        for attn_score, cross_attn_score, src, trg in zip(attn_scores, cross_attn_scores, src_decoded, trg_decoded):\n",
    "            self.draw_attention_map(attn_score, cross_attn_score, src, trg)\n",
    "\n",
    "        # 返回翻译结果，去除 BPE 标记\n",
    "        return [self.mose_detokenizer.tokenize(self.pattern.sub(\"\", s).split()) for s in self.zh_tokenizer.decode(preds)]\n",
    "\n",
    "\n",
    "# 示例输入句子列表（英文）\n",
    "sentence_list = [\"The secretary-general of the organization could provide support for the implementation of measures between parties, working towards security and assistance for children in need.\"]\n",
    "# \"In table 7, the Committee notes a marked difference in the figures for consultants in the International Tribunal for the Former Yugoslavia and the International Criminal Tribunal for Rwanda.\"\n",
    "# \"He stated that peacekeeping operations should focus on prevention rather than reaction to conflicts.\"\n",
    "# \"Ms. Maria emphasized that financial resources should be directed to regions facing economic challenges, particularly those with limited resources.\"\n",
    "# \"The secretary-general of the organization could provide support for the implementation of measures between parties, working towards security and assistance for children in need.\"\n",
    "# \"I am an old man who is very tired.\",\n",
    "# \"I have a new car, and it is very fast.\"\n",
    "\n",
    "# 加载模型并进行推理\n",
    "model = TransformerModel(config)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# 假设 en_tokenizer 和 zh_tokenizer 已经初始化\n",
    "translator = Translator(model.cpu(), en_tokenizer, zh_tokenizer)\n",
    "\n",
    "# 执行翻译并绘制热力图\n",
    "translated_sentences = translator(sentence_list, layer_idx=-1)\n",
    "\n",
    "# 输出翻译结果\n",
    "for sentence in translated_sentences:\n",
    "    print(\"Translated sentence:\", sentence)"
   ],
   "id": "f0d6fd1343aa89eb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
